from typing import List, Dict, Any, Optional
import json
import re
import time
import ast

try:
    import requests

    _HAS_REQUESTS = True
except Exception:
    _HAS_REQUESTS = False

try:
    import llama_cpp

    _HAS_LLAMA_CPP = True
except Exception:
    _HAS_LLAMA_CPP = False


class VikhrSlotExtractor:
    def __init__(self,
                 backend: str = "ollama",
                 ollama_url: str = "http://localhost:11434",
                 model_name: str = "wavecut/vikhr",
                 llama_model_path: Optional[str] = None,
                 llama_kwargs: Optional[Dict[str, Any]] = None):
        self.backend = backend
        self.ollama_url = ollama_url.rstrip("/")
        self.model_name = model_name
        self.llama_model_path = llama_model_path
        self.llama_kwargs = llama_kwargs or {}
        self._llama = None

        if self.backend == "llama_cpp" and not _HAS_LLAMA_CPP:
            raise RuntimeError("llama_cpp backend requested but 'llama-cpp-python' not installed")
        if self.backend == "ollama" and not _HAS_REQUESTS:
            raise RuntimeError("requests library required for ollama backend")

    def _clean_response(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç markdown-–æ–±–≤—è–∑–∫—É (```json ... ```) –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å."""
        text = text.strip()
        # –£–¥–∞–ª—è–µ–º ```json –≤ –Ω–∞—á–∞–ª–µ –∏ ``` –≤ –∫–æ–Ω—Ü–µ
        pattern = r"^```(?:json)?\s*(.*?)\s*```$"
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1)
        return text

    def _safe_json_extract(self, text: str) -> Optional[Dict[str, Any]]:
        # –°–Ω–∞—á–∞–ª–∞ —á–∏—Å—Ç–∏–º –æ—Ç –º–∞—Ä–∫–¥–∞—É–Ω–∞
        clean_text = self._clean_response(text)

        start = clean_text.find("{")
        if start == -1:
            return None

        depth = 0
        for i in range(start, len(clean_text)):
            char = clean_text[i]
            if char == "{":
                depth += 1
            elif char == "}":
                depth -= 1
                if depth == 0:
                    candidate = clean_text[start:i + 1]
                    try:
                        return json.loads(candidate)
                    except json.JSONDecodeError:
                        try:
                            # Fallback –¥–ª—è –æ–¥–∏–Ω–∞—Ä–Ω—ã—Ö –∫–∞–≤—ã—á–µ–∫
                            return ast.literal_eval(candidate)
                        except Exception:
                            return None
        return None

    def _coerce_types(self, slots_data: Dict[str, Any], slot_specs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∫ –Ω—É–∂–Ω—ã–º —Ç–∏–ø–∞–º (str -> int, str -> float)."""
        result = {}
        spec_map = {s["name"]: s.get("type", "str") for s in slot_specs}

        for key, value in slots_data.items():
            if key not in spec_map:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ (–ª–∏—à–Ω–∏–µ –∫–ª—é—á–∏)

            target_type = spec_map[key]

            if value is None:
                result[key] = None
                continue

            try:
                if target_type == "int":
                    result[key] = int(float(str(value).replace(",", ".")))  # "5.0" -> 5
                elif target_type == "float":
                    result[key] = float(str(value).replace(",", "."))
                elif target_type == "bool":
                    s_val = str(value).lower()
                    result[key] = s_val in ["true", "1", "yes", "–¥–∞", "t"]
                else:
                    result[key] = str(value)
            except Exception:
                # –ï—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø—Ä–∏–≤–µ—Å—Ç–∏ —Ç–∏–ø, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (–∏–ª–∏ –º–æ–∂–Ω–æ —Å—Ç–∞–≤–∏—Ç—å None)
                result[key] = value

        return result

    def _build_prompt(self, intent: str, text: str, slot_specs: List[Dict[str, Any]], example: Optional[str]) -> str:
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–≥—É—é —Å—Ö–µ–º—É
        schema_desc = {}
        for s in slot_specs:
            schema_desc[s["name"]] = f"Type: {s.get('type', 'str')}, Required: {s.get('required', False)}"

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        safe_text = json.dumps(text, ensure_ascii=False)
        safe_schema = json.dumps(schema_desc, ensure_ascii=False, indent=2)

        prompt = f"""### Instruction:
You are a smart NLU (Natural Language Understanding) system.
Analyze the User Command and extract arguments (slots) according to the Schema.

Intent: "{intent}"
Schema:
{safe_schema}

Rules:
1. Return ONLY a JSON object. No explanations.
2. If a required argument is missing, try to infer it from context. If impossible, use null.
3. If an optional argument is missing, use null.
4. Do NOT translate values unless necessary.

"""
        if example:
            prompt += f"Example Command: {example}\n"

        prompt += f"""
### User Command:
{safe_text}

### Response (JSON):
"""
        return prompt

    # ----- Backend Calls -----

    def _call_ollama(self, prompt: str, max_tokens: int = 256, temperature: float = 0.1) -> str:
        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∏–∂–µ (0.1) –¥–ª—è –±–æ–ª—å—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        url = f"{self.ollama_url}/api/generate"
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "format": "json"  # Ollama –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–∏–µ JSON
        }
        try:
            r = requests.post(url, json=payload, timeout=20)  # –ß—É—Ç—å –±–æ–ª—å—à–µ —Ç–∞–π–º–∞—É—Ç
            r.raise_for_status()
            data = r.json()
            return data.get("response", "")
        except Exception as e:
            raise RuntimeError(f"Ollama call failed: {e}")

    def _init_llama(self):
        if self._llama is not None:
            return
        self._llama = llama_cpp.Llama(model_path=self.llama_model_path, **self.llama_kwargs)

    def _call_llama(self, prompt: str, max_tokens: int = 256, temperature: float = 0.1) -> str:
        self._init_llama()
        # Llama-cpp –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ create_completion —Å –≥—Ä–∞–º–º–∞—Ç–∏–∫–æ–π, –Ω–æ —ç—Ç–æ —Å–ª–æ–∂–Ω–æ.
        # –û—Å—Ç–∞–≤–∏–º –ø—Ä–æ—Å—Ç–æ–π –≤—ã–∑–æ–≤, –Ω–æ —Å–Ω–∏–∑–∏–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É.
        resp = self._llama.create_completion(
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["###", "User Command"]  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ —á—Ç–æ–±—ã –Ω–µ –±—Ä–µ–¥–∏–ª
        )
        return resp.get("choices", [{}])[0].get("text", "")

    # ----- Main Method -----

    def extract_slots(self,
                      intent_name: str,
                      normalized_text: str,
                      slot_specs: List[Dict[str, Any]],
                      example: Optional[str] = None,
                      max_tokens: int = 256,
                      temperature: float = 0.1) -> Dict[str, Any]:

        prompt = self._build_prompt(intent_name, normalized_text, slot_specs, example)
        start = time.time()

        raw_response = ""
        try:
            if self.backend == "ollama":
                raw_response = self._call_ollama(prompt, max_tokens, temperature)
            elif self.backend == "llama_cpp":
                raw_response = self._call_llama(prompt, max_tokens, temperature)
            else:
                raise RuntimeError(f"Unknown backend {self.backend}")
        except Exception as e:
            return {"ok": False, "slots": {}, "raw": "", "explain": f"model error: {e}"}

        duration = time.time() - start

        # –ü–∞—Ä—Å–∏–º JSON
        parsed_data = self._safe_json_extract(raw_response)

        if parsed_data is None:
            # Fallback –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∏—Å–ª–∞ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ç—É–ø–∞—è –∏ –≤–µ—Ä–Ω—É–ª–∞ –ø—Ä–æ—Å—Ç–æ "5")
            if len(slot_specs) == 1:
                single_slot = slot_specs[0]
                name = single_slot["name"]
                # –ò—â–µ–º —á–∏—Å–ª–æ
                m = re.search(r"-?\d+([.,]\d+)?", raw_response)
                if m:
                    val_str = m.group(0).replace(",", ".")
                    try:
                        val = float(val_str) if "." in val_str else int(val_str)
                        return {
                            "ok": True,
                            "slots": {name: val},
                            "raw": raw_response,
                            "explain": "fallback regex number",
                            "duration": duration
                        }
                    except:
                        pass

            return {
                "ok": False,
                "slots": {},
                "raw": raw_response,
                "explain": "failed to parse JSON",
                "duration": duration
            }

        # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ª–∏—à–Ω–µ–µ
        final_slots = self._coerce_types(parsed_data, slot_specs)

        return {
            "ok": True,
            "slots": final_slots,
            "raw": raw_response,
            "explain": "parsed json",
            "duration": duration
        }

# ---------------- Demo ----------------
if __name__ == "__main__":
    print("--- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Extractor ---")
    # –£–±–µ–¥–∏—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞!
    ext = VikhrSlotExtractor(
        backend="ollama",
        model_name="llama3:8b",  # –ò–ª–∏ –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
        ollama_url="http://localhost:11434"
    )

    test_cases = [
        {
            "name": "–ü—Ä–æ—Å—Ç–æ–π —Ç–∞–π–º–µ—Ä",
            "intent": "set_timer",
            "text": "–ø–æ—Å—Ç–∞–≤—å —Ç–∞–π–º–µ—Ä –Ω–∞ 10 —Å–µ–∫—É–Ω–¥",
            "slots": [{"name": "minutes", "type": "int"}, {"name": "seconds", "type": "int"}]
        },
        {
            "name": "–¢–∞–π–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–º (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤)",
            "intent": "set_timer",
            "text": "—Ç–∞–π–º–µ—Ä –Ω–∞ –ø—è—Ç—å –º–∏–Ω—É—Ç",
            "slots": [{"name": "minutes", "type": "int"}]
        },
        {
            "name": "–°–≤–µ—Ç (Enum/String)",
            "intent": "turn_on_light",
            "text": "–≤–∫–ª—é—á–∏ —Å–≤–µ—Ç –Ω–∞ –∫—É—Ö–Ω–µ",
            "slots": [{"name": "location", "type": "str", "required": True}, {"name": "color", "type": "str"}]
        },
        {
            "name": "–ù–µ–ø–æ–ª–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ (–ø—Ä–æ–≤–µ—Ä–∫–∞ null)",
            "intent": "set_timer",
            "text": "—Ç–∞–π–º–µ—Ä",
            "slots": [{"name": "minutes", "type": "int", "required": True}]
        }
    ]

    for case in test_cases:
        print(f"\nüß™ –¢–µ—Å—Ç: {case['name']}")
        print(f"   –í—Ö–æ–¥: '{case['text']}'")

        res = ext.extract_slots(
            intent_name=case["intent"],
            normalized_text=case["text"],
            slot_specs=case["slots"]
        )

        if res["ok"]:
            print(f"   –£—Å–ø–µ—Ö: {res['slots']}")
        else:
            print(f"   –û—à–∏–±–∫–∞: {res['explain']}")
            print(f"   Raw: {res['raw']}")
